# SargamAI

**Where content meets clarity.**

SargamAI is a production-grade Retrieval-Augmented Generation (RAG) application that transforms PDFs, YouTube videos, and CSV datasets into interactive, conversational knowledge bases. Built with **FastAPI** and **Next.js**, powered by state-of-the-art language models via OpenRouter, it features a modern terminal-inspired UI with PDF split-view, intelligent multi-modal querying, persistent chat history, and context-aware responses.

[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/fastapi-latest-green.svg)](https://fastapi.tiangolo.com/)
[![Next.js](https://img.shields.io/badge/next.js-16.1-black.svg)](https://nextjs.org/)
[![LangChain](https://img.shields.io/badge/langchain-1.2+-green.svg)](https://python.langchain.com/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [How It Works](#-how-it-works)
- [Memory System](#-memory-system)
- [Data Schema](#-data-schema)
- [Security Considerations](#-security-considerations)

---

## âœ¨ Features

### Core Capabilities
- **Secure Authentication** - Firebase-backed user management with email/password authentication
- **PDF Processing** - Upload and automatically index PDF documents for semantic search with split-view display
- **YouTube Analysis** - Paste any YouTube URL to extract transcript, index it, and chat about the video
- **CSV Intelligence** - Upload CSV datasets and query them with natural language via a Pandas agent
- **Conversational RAG** - Ask natural language questions about your content with context-aware responses
- **Multi-Model Support** - Access GPT-4, Claude, Gemini, and Grok models through a unified interface
- **Persistent Chat History** - Conversations survive refreshes and are stored in Firestore
- **Auto-Load Intelligence** - Vectorstores load automatically when switching between documents
- **User Profiles** - Customizable display names and saved API keys per user

### UI/UX Features
- **Terminal-Inspired Thinking State** - Visual pipeline stages (PARSE â†’ EMBED â†’ SEARCH â†’ RANK â†’ GEN)
- **PDF Split-View** - Document viewer on left, chat interface on right for PDF files
- **Collapsible Source Chunks** - View retrieved document chunks with page numbers and excerpts
- **Modern Design System** - "Obsidian Ember" theme with custom Fontshare fonts (Satoshi, Clash Display, General Sans, JetBrains Mono)
- **Responsive & Fast** - Built with Next.js 16 + React 19 + Tailwind CSS v4

### Technical Highlights
- **FastAPI Backend** - Async REST API with JWT authentication
- **Next.js Frontend** - Server-side rendering, App Router, Turbopack
- **Multi-Modal Pipeline** - Unified RAG architecture handles PDFs, YouTube transcripts, and CSV datasets
- **Semantic Chunking** - Intelligent text splitting preserving context across 1000-character segments
- **Local Embeddings** - HuggingFace `all-MiniLM-L6-v2` runs locally (no API costs)
- **FAISS Vector Store** - High-performance similarity search with Firestore persistence (chunked <700KB)
- **Pandas Agent** - Natural language querying of structured data via LangChain agents
- **Conversational Memory** - Windowed memory tracks last 8 exchanges for context retention
- **Two-Stage Retrieval** - Condense-question chain + document QA chain for accurate responses
- **Cloud Ready** - Deploy backend on Render, frontend on Vercel

---

## ğŸ— Architecture

OmniMind implements a modern, production-ready full-stack RAG architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     NEXT.JS FRONTEND (CLIENT/)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ app/login    â”‚  â”‚ app/chat     â”‚  â”‚ app/profile  â”‚             â”‚
â”‚  â”‚ (Auth UI)    â”‚  â”‚ (Split-view) â”‚  â”‚ (Settings)   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚  â€¢ NextJS                  â€¢ Tailwind CSS v4                      â”‚
â”‚  â€¢ Firebase Client SDK      â€¢ Terminal Aesthetic                  â”‚
â”‚  â€¢ PDF Split-View           â€¢ Source Chunks Display               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ â†‘ (REST API + JWT)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASTAPI BACKEND (SERVER/)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ routes/                                                   â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ auth.py       (Register endpoint)                    â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ upload.py     (PDF/YouTube/CSV ingestion)            â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ chat.py       (Message endpoint, returns sources)    â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ files.py      (List, delete, GET PDF bytes)          â”‚    â”‚
â”‚  â”‚  â””â”€â”€ profile.py    (User settings, API key)               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ modules/                                                  â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ chains.py     (LCEL-based ConversationalRAGChain)    â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ rag.py        (Vectorstore creation, chunking)       â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ memory.py     (Chat history management)              â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ database.py   (Firestore operations, PDF storage)    â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ agents.py     (Calculator, Wikipedia, DuckDuckGo)    â”‚    â”‚
â”‚  â”‚  â””â”€â”€ llm.py        (OpenRouter client)                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FIREBASE BACKEND                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚Firebase Auth     â”‚         â”‚Firestore DB      â”‚               â”‚
â”‚  â”‚â€¢ Secure login    â”‚         â”‚â€¢ Vectorstores    â”‚               â”‚
â”‚  â”‚â€¢ JWT tokens      â”‚         â”‚â€¢ Chat history    â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚â€¢ User profiles   â”‚               â”‚
â”‚                               â”‚â€¢ Raw PDFs        â”‚               â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXTERNAL SERVICES                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ OpenRouter   â”‚  â”‚ HuggingFace  â”‚  â”‚   FAISS      â”‚            â”‚
â”‚  â”‚ (LLM Access) â”‚  â”‚ (Embeddings) â”‚  â”‚(Vector Searchâ”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Request Flow

```
User Query â†’ Content Type Router
              â”œâ”€ PDF / YouTube (RAG mode)
              â”‚    â†“
              â”‚  Condense-Question Chain
              â”‚    â†“
              â”‚  Chat History (last 8 turns)
              â”‚    â†“
              â”‚  Standalone Query
              â”‚    â†“
              â”‚  FAISS Similarity Search (k=3)
              â”‚    â†“
              â”‚  Retrieved Chunks (returned to frontend)
              â”‚    â†“
              â”‚  Stuff-Docs QA Chain
              â”‚    â†“
              â”‚  LLM Response (via OpenRouter)
              â”‚
              â””â”€ CSV (Agent mode)
                   â†“
                 Pandas DataFrame Agent
                   â†“
                 Natural Language â†’ Python Execution
                   â†“
                 Computed Result / Plot
              â†“
  Memory Update + Firestore Persist
```

---

## ğŸ›  Tech Stack

### Backend (server/)
| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Framework** | FastAPI + uvicorn[standard] | Async REST API with auto docs |
| **LLM Framework** | LangChain 1.2+ (LCEL) | RAG chains, agents, memory |
| **LLM Provider** | OpenRouter | Access to 100+ models (GPT, Claude, etc.) |
| **Embeddings** | HuggingFace (all-MiniLM-L6-v2) | Sentence encoding (384-dim vectors) |
| **Vector Store** | FAISS (faiss-cpu) | Fast similarity search (in-memory) |
| **Database** | Firebase Firestore | NoSQL for user data, chat, vectorstores, PDFs |
| **Authentication** | Firebase Admin SDK | JWT token verification |
| **Data Processing** | PyPDF, pandas, youtube-transcript-api | PDF/CSV/YouTube parsing |

### Frontend (client/)
| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Framework** | Next.js 16.1.6 | React framework with App Router |
| **UI Library** | React 19.2.3 | Component-based UI |
| **Styling** | Tailwind CSS v4 | Utility-first CSS framework |
| **Authentication** | Firebase Client SDK | User auth state management |
| **Fonts** | Fontshare (Satoshi, Clash Display, General Sans, JetBrains Mono) | Custom typography |
| **Build Tool** | Turbopack | Fast bundler for Next.js |

---

## ğŸ“‹ Prerequisites

### Required Accounts
1. **Firebase Project** ([console.firebase.google.com](https://console.firebase.google.com))
   - Enable Authentication (Email/Password provider)
   - Create a Firestore database
   - Generate a service account key (JSON) and save as `serviceAccount.json` in project root

2. **OpenRouter Account** ([openrouter.ai](https://openrouter.ai))
   - Sign up for an API key
   - Fund account (pay-as-you-go pricing)

### Required Software
- **Python 3.11+** (backend)
- **Node.js 18+** (frontend)
- **npm or yarn** (package manager)

---

## ğŸš€ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/omnimind.git
cd omnimind
```

### 2. Backend Setup

#### Create Virtual Environment

```bash
cd server
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

#### Install Python Dependencies

```bash
pip install -r requirements.txt
```

#### Download Embedding Model

The HuggingFace embedding model will auto-download on first run (~90MB). To pre-cache:

```bash
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"
```

### 3. Frontend Setup

```bash
cd ../client
npm install
```

---

## âš™ Configuration

### 1. Firebase Setup

Place your `serviceAccount.json` in the project root (`OmniMind/serviceAccount.json`).

### 2. Backend Configuration

Create `server/.env` (optional, for custom ports):

```env
PORT=8000
```

### 3. Frontend Configuration

Create `client/.env.local`:

```env
NEXT_PUBLIC_API_URL=http://localhost:8000
NEXT_PUBLIC_FIREBASE_API_KEY=AIzaSXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
NEXT_PUBLIC_FIREBASE_AUTH_DOMAIN=your-project.firebaseapp.com
NEXT_PUBLIC_FIREBASE_PROJECT_ID=your-project-id
```

Get Firebase config from: Firebase Console â†’ Project Settings â†’ General â†’ Your apps â†’ Firebase SDK snippet

### 4. OpenRouter API Key

Users provide their own OpenRouter API keys via the Profile page (stored in Firestore per-user).

---

## ğŸ® Usage

### Starting the Application

#### Terminal 1 - Backend

```bash
cd server
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

Backend will run at `http://localhost:8000` (API docs at `/docs`)

#### Terminal 2 - Frontend

```bash
cd client
npm run dev
```

Frontend will run at `http://localhost:3000`

### First-Time Setup

1. **Create Account**
   - Open `http://localhost:3000/login`
   - Click "Sign Up" tab
   - Enter email, password, and display name
   - Click "Sign Up"

2. **Configure API Key**
   - Navigate to Profile page
   - Enter your OpenRouter API key
   - Click "Save API Key"

### Uploading Content

OmniMind supports three content types via tabs in the Upload panel:

#### ğŸ“„ PDF Documents
1. Open the **PDF** tab in the sidebar
2. Click "Choose a PDF" and select a file
3. Click "Upload PDF"
4. Wait for processing (text extraction â†’ chunking â†’ embedding â†’ Firestore storage)
5. Select the file from "Your Files" to start chatting
6. **PDF Split-View**: When a PDF is selected, the document appears on the left, chat on the right

#### ğŸ¥ YouTube Videos
1. Open the **YouTube** tab
2. Paste a YouTube URL (e.g., `https://www.youtube.com/watch?v=dQw4w9WgXcQ`)
3. Click "Upload YouTube"
4. Transcripts are extracted, chunked, and indexed
5. File appears as `yt_{video_id}` in "Your Files"

#### ğŸ“Š CSV Files
1. Open the **CSV** tab
2. Upload a CSV file (must have headers)
3. The file is loaded into a Pandas DataFrame
4. Use natural language to query (e.g., "What's the average sales?", "Plot revenue by month")

### Chatting

1. Select a file from "Your Files" list
2. Type your question in the input box at the bottom
3. **Thinking State**: Watch the terminal-style pipeline stages as the system processes your query:
   - `[+] PARSE` - Question condensation
   - `[+] EMBED` - Vector embedding
   - `[+] SEARCH` - FAISS similarity search
   - `[+] RANK` - Relevance scoring
   - `[+] GEN` - Response generation
4. **Source Chunks**: Click the collapsible section to view retrieved document chunks with page numbers
5. **Context**: Last 8 Q&A pairs are automatically included for context-aware responses

### Autonomous Agents

If RAG can't answer (e.g., "What's 123 * 456?" or "Who won the 2024 Olympics?"), the system falls back to LangChain agents:
- **Calculator** - Math queries
- **Wikipedia** - General knowledge
- **DuckDuckGo** - Current events

---

## ğŸ“ Project Structure

```
OmniMind/
â”œâ”€â”€ server/                          # FastAPI Backend
â”‚   â”œâ”€â”€ main.py                      # FastAPI app entry point
â”‚   â”œâ”€â”€ middleware.py                # JWT authentication
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ auth.py                  # Register endpoint
â”‚   â”‚   â”œâ”€â”€ upload.py                # PDF/YouTube/CSV upload
â”‚   â”‚   â”œâ”€â”€ chat.py                  # Message endpoint
â”‚   â”‚   â”œâ”€â”€ files.py                 # File management, PDF serving
â”‚   â”‚   â””â”€â”€ profile.py               # User profile, API key
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ chains.py                # LCEL-based ConversationalRAGChain
â”‚   â”‚   â”œâ”€â”€ rag.py                   # Vectorstore creation
â”‚   â”‚   â”œâ”€â”€ memory.py                # Chat history
â”‚   â”‚   â”œâ”€â”€ database.py              # Firestore operations
â”‚   â”‚   â”œâ”€â”€ agents.py                # LangChain agents
â”‚   â”‚   â”œâ”€â”€ llm.py                   # OpenRouter client
â”‚   â”‚   â”œâ”€â”€ prompts.py               # System prompts
â”‚   â”‚   â””â”€â”€ theme.py                 # (Legacy, not used)
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ client/                          # Next.js Frontend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ layout.tsx               # Root layout
â”‚   â”‚   â”œâ”€â”€ page.tsx                 # Landing page
â”‚   â”‚   â”œâ”€â”€ login/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx             # Auth page
â”‚   â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx             # Main chat interface (split-view)
â”‚   â”‚   â”œâ”€â”€ profile/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx             # User profile
â”‚   â”‚   â””â”€â”€ globals.css              # Obsidian Ember theme
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ firebase.ts              # Firebase client config
â”‚   â”‚   â”œâ”€â”€ auth-context.tsx         # Auth state management
â”‚   â”‚   â””â”€â”€ api.ts                   # API client functions
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.ts
â”‚
â”œâ”€â”€ serviceAccount.json              # Firebase Admin SDK credentials
â”œâ”€â”€ firebase.config                  # (Legacy, not used)
â””â”€â”€ README.md
```

---

## ğŸ§  How It Works

### Complete RAG Pipeline

#### 1. **Document Ingestion**

When a user uploads a PDF:

```python
# Extract text
loader = PyPDFLoader(file_path)
pages = loader.load()

# Chunk with overlap
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " ", ""]
)
chunks = text_splitter.split_documents(pages)
```

**Why 1000 chars?**
- Short enough to fit in context windows
- Long enough to preserve semantic meaning
- 200-char overlap prevents context loss at boundaries

#### 2. **Embedding Generation**

```python
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
# Generates 384-dimensional vectors per chunk
# Runs locally (no API costs, ~1-2s per document)
```

**Model Properties:**
- **Dimensions:** 384 (smaller = faster search)
- **Speed:** ~500 sentences/second on CPU
- **Accuracy:** 69.57 on MTEB benchmark
- **License:** Apache 2.0 (commercial-friendly)

#### 3. **Vector Storage**

```python
# Create in-memory FAISS index
vectorstore = FAISS.from_documents(chunks, embeddings)

# Serialize and chunk for Firestore
pkl = vectorstore.serialize_to_bytes()
chunks = [pkl[i:i+700KB] for i in range(0, len(pkl), 700KB)]

# Store in Firestore subcollection
users/{uid}/files/{filename}/chunks/{0,1,2,...}

# Also store raw PDF bytes
users/{uid}/files/{filename}/pdf_raw/{0,1,2,...}
```

**Why FAISS?**
- **Fast:** ~1ms for similarity search on 10K vectors
- **Efficient:** Low memory footprint
- **Portable:** Serializes to bytes for cloud storage

**Why chunk at 700KB?**
- Firestore document size limit is 1MB
- 700KB provides safety margin for metadata

#### 4. **Query Processing**

When a user asks a question, the system executes:

**Stage 1: Condense Question (if chat history exists)**

```python
# Prompt template:
Given the following conversation history and a follow-up question,
rephrase the follow-up question into a standalone question.

Chat History:
{last 8 turns}

Follow-Up Question: {user_query}
Standalone Question: [LLM output]
```

**Example:**
```
History:
  User: "What is machine learning?"
  AI: "Machine learning is a subset of AI that enables systems to learn..."

User: "How does it differ from deep learning?"

Condense Chain Output: "How does machine learning differ from deep learning?"
```

**Stage 2: Retrieval**

```python
# Embed standalone query
query_vector = embeddings.embed_query(standalone_question)

# FAISS similarity search
docs = vectorstore.similarity_search(query_vector, k=3)
# Returns top 3 most relevant chunks
```

**Stage 3: Answer Generation**

```python
# System prompt template:
You are OmniMind, a helpful and knowledgeable AI assistant.
Use the following pieces of retrieved context to answer the user's question.

Context:
{concatenated chunks from retrieval}

User: {standalone_question}
```

**Stage 4: Return to Frontend**

```json
{
  "answer": "Machine learning differs from deep learning in that...",
  "sources": [
    {
      "content": "Machine learning is a subset...",
      "page": 12,
      "source": "ml_textbook.pdf"
    },
    ...
  ]
}
```

The frontend displays the answer and shows collapsible source chunks with page numbers.

#### 5. **Memory Update**

```python
# Add Q&A pair to windowed memory
memory.save_context(
    {"question": query},
    {"answer": response}
)
# Oldest turn dropped when window size (k=8) exceeded

# Persist to Firestore
save_chat_message(user_id, file_name, "user", query)
save_chat_message(user_id, file_name, "assistant", response)
```

---

## ğŸ’¾ Memory System

### Architecture

OmniMind implements a **dual-layer memory system**:

| Layer | Storage | Lifespan | Purpose |
|-------|---------|----------|---------|
| **LangChain Memory** | Backend state | Session only | Fed into chain for context-aware retrieval |
| **Display History** | Firestore | Persistent | Shown in UI, survives refresh |

### Memory Type: Windowed Message History

```python
def build_memory_from_history(history: list) -> list:
    """Convert chat history to LangChain message objects."""
    messages = []
    for msg in history[-8:]:  # Last 8 turns
        if msg["role"] == "user":
            messages.append(HumanMessage(content=msg["content"]))
        else:
            messages.append(AIMessage(content=msg["content"]))
    return messages
```

**Why windowed (not summary-based)?**

Initial implementation used `ConversationSummaryBufferMemory`, but OpenRouter-proxied models lack the `get_num_tokens_from_messages()` method required for pruning decisions. The windowed approach is simpler and doesn't require token counting.

**Per-File Isolation**

Each document gets its own chat history in Firestore:
```
users/{uid}/files/{filename}/messages/{auto-id}
```

This prevents context bleeding between different documents.

### Persistence Flow

```
New Message
    â†“
Frontend State Update (instant)
    â†“
POST /chat (backend processes)
    â†“
Firestore Write
    â†“
users/{uid}/files/{file}/messages/{auto-id}
  { role: "user", content: "...", timestamp: <server> }
```

On page reload:
```
Page Load
    â†“
GET /files/{file_name}/messages
    â†“
Load from Firestore
    â†“
Display in UI
```

---

## ğŸ“Š Data Schema

### Firestore Collections

```
users/
  {user_id}/                          # Firebase Auth UID
    â”œâ”€â”€ email: string                 # User's email address
    â”œâ”€â”€ username: string              # Display name (editable)
    â”œâ”€â”€ api_key: string               # OpenRouter API key (encrypted at rest)
    â”‚
    â””â”€â”€ files/
          {filename}/                 # e.g. "whitepaper.pdf", "yt_dQw4w9WgXcQ", "sales.csv"
            â”œâ”€â”€ file_name: string
            â”œâ”€â”€ content_type: string  # "pdf" | "youtube" | "csv"
            â”œâ”€â”€ total_chunks: number  # Number of FAISS binary chunks (PDF/YouTube)
            â”œâ”€â”€ total_size: number    # Original vectorstore size (bytes)
            â”œâ”€â”€ dataframe: bytes      # Pickled DataFrame (CSV only)
            â”œâ”€â”€ created_at: timestamp
            â”‚
            â”œâ”€â”€ chunks/               # FAISS vectorstore (serialized, chunked)
            â”‚     0/
            â”‚       â”œâ”€â”€ data: bytes   # Binary chunk (â‰¤700KB)
            â”‚       â””â”€â”€ chunk_id: number
            â”‚     1/
            â”‚       â”œâ”€â”€ data: bytes
            â”‚       â””â”€â”€ chunk_id: number
            â”‚     ...
            â”‚
            â”œâ”€â”€ pdf_raw/              # Raw PDF bytes (for split-view display)
            â”‚     0/
            â”‚       â”œâ”€â”€ data: bytes   # Binary chunk (â‰¤700KB)
            â”‚       â””â”€â”€ chunk_id: number
            â”‚     ...
            â”‚
            â””â”€â”€ messages/             # Chat history
                  {auto-id}/
                    â”œâ”€â”€ role: "user" | "assistant"
                    â”œâ”€â”€ content: string
                    â””â”€â”€ timestamp: timestamp
                  ...
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Built with â¤ï¸ by Anas**

*For questions or support, open an issue on GitHub or reach out via email.*
